{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExamplePySparkNotebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka topic\n",
    "input_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"input_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Split the input text into words\n",
    "words = input_stream.select(explode(split(input_stream.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Perform word count\n",
    "word_count = words.groupBy(\"word\").count()\n",
    "\n",
    "# Write the result to HDFS\n",
    "word_count.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/hdfs_output/word_count\") \\\n",
    "    .option(\"checkpointLocation\", \"/hdfs_output/checkpoints\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
